{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_taO1jmRpRFJ",
    "tags": []
   },
   "source": [
    "# EE 467 Lab 2: Breaking CAPTCHAs with Tensorflow / Keras\n",
    "In lab 2, we are going to work on automatically breaking CAPTCHAs with deep learning! Originally, CAPTCHA stands for \"Completely Automated Public Turing test to tell Computers and Humans Apart\", and traditional CAPCTHAs serve as a great tool to stop spam bots and malicious crawlers. Today, as we have made huge progress in computer vision and deep learning, these CAPCTHAs are no longer unbreakable by computers. Now let's prove the correctness of the claim by ourselves!\n",
    "\n",
    "As usual, please check if the helper library, `lab_2_helpers.py` and the extracted dataset directory, `captcha-images` exist under the same directory. Then, please install all libraries used for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JT15OpCGpRFK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: opencv-python>4 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: imutils in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (0.5.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow>2 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (2.18.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorflow>2) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from ml-dtypes<1.0.0,>=0.4.0->tensorflow>2) (2.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>2) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow>2) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow>2) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow>2) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow>2) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow>2) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow>2) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow>2) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>2) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow>2) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow>2) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/anaconda-ml-ai/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>2) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib scikit-learn \"opencv-python>4\" imutils\n",
    "\n",
    "# Install Tensorflow with CPU support only, or ...\n",
    "%pip install \"tensorflow>2\"\n",
    "# Install Tensorflow with GPU support:\n",
    "#%pip install tensorflow-gpu>2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jg5BVuIipRFL"
   },
   "source": [
    "Next, we import all tools needed before starting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "en7AeT1vpRFL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 13:59:35.643732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimutils\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m paths\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, layers\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgridspec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSpec\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import os, pickle, glob, math\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "from imutils import paths\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lab_3_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcvW2zepRFL"
   },
   "source": [
    "# Preprocessing\n",
    "## Ground Truth Characters Extraction\n",
    "As usual, we will start pre-processing stage by loading CAPTCHA images into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wzjdfESEqKDW"
   },
   "outputs": [],
   "source": [
    "!tar -xJf captcha-images.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O91YlRgdpRFL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m CAPTCHA_IMAGE_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./captcha-images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# List of all the captcha images we need to process\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m captcha_image_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(paths\u001b[38;5;241m.\u001b[39mlist_images(CAPTCHA_IMAGE_FOLDER))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Review image paths\u001b[39;00m\n\u001b[1;32m      7\u001b[0m pprint(captcha_image_paths[:\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paths' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset images folder\n",
    "CAPTCHA_IMAGE_FOLDER = \"./captcha-images\"\n",
    "\n",
    "# List of all the captcha images we need to process\n",
    "captcha_image_paths = list(paths.list_images(CAPTCHA_IMAGE_FOLDER))\n",
    "# Review image paths\n",
    "pprint(captcha_image_paths[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_W0sI_hpRFL"
   },
   "source": [
    "Note that for each image, its file name (without extension) happens to be its corresponding CAPTCHA text. Thus, we extract file names for all CAPTCHA images and save them as labels for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUTyEunkpRFL"
   },
   "outputs": [],
   "source": [
    "def extract_captcha_text(image_path):\n",
    "    \"\"\" Extract correct CAPTCHA texts from file name of images. \"\"\"\n",
    "    # Extract file name of image from its path\n",
    "    # e.g. \"./captcha-images/2A2X.png\" -> \"2A2X.png\"\n",
    "    image_file_name = os.path.basename(image_path)\n",
    "    # Extract base name of image, omitting file extension\n",
    "    # e.g. \"2A2X.png\" -> \"2A2X\"\n",
    "    return os.path.splitext(image_file_name)[0]\n",
    "\n",
    "captcha_texts = [extract_captcha_text(image_path) for image_path in captcha_image_paths]\n",
    "# Review extraction results\n",
    "pprint(captcha_texts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSmTPs95pRFL"
   },
   "source": [
    "## Loading and Transforming Images\n",
    "For the feature extraction stage, we are going to extract individual characters from these CAPTCHAs. This is done by looking for contours (bounding boxes) around characters, then cropping the CAPTCHAs such as only the contour areas are preserved. We begin feature extraction by loading and transforming images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALywLnsspRFM"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def load_transform_image(image_path):\n",
    "    \"\"\" Load and transform image into grayscale. \"\"\"\n",
    "    ## [ TODO ]\n",
    "    # 1) Load image with OpenCV\n",
    "    image = NotImplemented\n",
    "\n",
    "    # 2) Convert image to grayscale\n",
    "    image_gray = NotImplemented\n",
    "    # 3) Add extra padding (8px) around the image\n",
    "    image_padded = NotImplemented\n",
    "\n",
    "    return image_padded\n",
    "\n",
    "captcha_images = [load_transform_image(image_path) for image_path in captcha_image_paths]\n",
    "# Review loaded CAPTCHAs\n",
    "print_images(\n",
    "    captcha_images[:10], n_rows=2, texts=captcha_texts[:10]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZTOQpM8pRFM"
   },
   "source": [
    "Next, we will split our dataset into train-validation set and test set. The former set will be used for training and validation in deep character classification model, while the latter will be used for testing our CAPTCHA recognition pipline end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiWpkO-LpRFM"
   },
   "outputs": [],
   "source": [
    "# Train-validation-test split seed\n",
    "TVT_SPLIT_SEED = 31528476\n",
    "\n",
    "# Perform split on CAPTCHA images as well as labels\n",
    "captcha_images_tv, captcha_images_test, captcha_texts_tv, captcha_texts_test = train_test_split(\n",
    "    captcha_images, captcha_texts, test_size=0.2, random_state=TVT_SPLIT_SEED\n",
    ")\n",
    "\n",
    "print(\"Train-validation:\", len(captcha_texts_tv))\n",
    "print(\"Test:\", len(captcha_texts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UquyiV3wpRFM"
   },
   "source": [
    "## Bounding Box Extraction\n",
    "It's now time to perform the most important feature extraction step: finding contours and extracting characters. Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. It is useful for shape analysis and object detection and recognition. For our task however, we are **more interested in the bounding boxes around characters**, since these are the part of images we will be used for character classification:\n",
    "\n",
    "![char-contours.png](attachment:char-contours.png)\n",
    "\n",
    "Here are the steps we are going to perform:\n",
    "\n",
    "* Find contours around characters in CAPTCHAs.\n",
    "* Get the position and size for each corresponding bounding box.\n",
    "* If a bounding box is too wide (width-to-height ratio larger than 1.25), chances are that we have bounded two letters in a single bounding box. In this case, split the bounding box vertically from the center into two.\n",
    "* Store all bounding boxes for the CAPTCHA.\n",
    "* If there aren't 4 bounding boxes for the CAPTCHA, ignore it since our character extraction process must have run into problems in this case.\n",
    "* Sort the bounding boxes by their X coordinates, so that they match the order corresponding letters occur.\n",
    "* For each bounding box, extract corresponding region of the image, and store it as an instance of corresponding character at `${CHAR_IMAGE_FOLDER}/{letter}/{count}.png`.\n",
    "\n",
    "After these steps, we have transformed CAPTCHA images into images of single character. This simplifies our task since now our model only needs to deal with classification (from character image to character itself) rather than also dealing with detection (finding and extracting charatcers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31GKyxSapRFM"
   },
   "outputs": [],
   "source": [
    "# Character images folder template\n",
    "CHAR_IMAGE_FOLDER = f\"./char-images-{TVT_SPLIT_SEED}\"\n",
    "\n",
    "def extract_chars(image):\n",
    "    \"\"\" Find contours and extract characters inside each CAPTCHA. \"\"\"\n",
    "    # Threshold image and convert it to black-white\n",
    "    image_bw = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    # Find contours (continuous blobs of pixels) the image\n",
    "    contours = cv2.findContours(image_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    char_regions = []\n",
    "    # Loop through each contour\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Compare the width and height of the bounding box,\n",
    "        # detect if there are letters conjoined into one chunk\n",
    "        if w / h > 1.25:\n",
    "            # Bounding box is too wide for a single character\n",
    "            # Split it in half into two letter regions\n",
    "            half_width = int(w / 2)\n",
    "            char_regions.append((x, y, half_width, h))\n",
    "            char_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            # Only a single letter in contour\n",
    "            char_regions.append((x, y, w, h))\n",
    "\n",
    "    # Ignore image if less or more than 4 regions detected\n",
    "    if len(char_regions)!=4:\n",
    "        return None\n",
    "    # Sort regions by their X coordinates\n",
    "    char_regions.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Character images\n",
    "    char_images = []\n",
    "    # Save each character as a single image\n",
    "    for x, y, w, h in char_regions:\n",
    "        # Extract character from image with 2px margin\n",
    "        char_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "        # Save character images\n",
    "        char_images.append(char_image)\n",
    "\n",
    "    # Return character images\n",
    "    return char_images\n",
    "\n",
    "def save_chars(char_images, captcha_text, save_dir, char_counts):\n",
    "    \"\"\" Save character images to directory. \"\"\"\n",
    "    for char_image, char in zip(char_images, captcha_text):\n",
    "        # Get the folder to save the image in\n",
    "        save_path = os.path.join(save_dir, char)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Write letter image to file\n",
    "        char_count = char_counts.get(char, 1)\n",
    "        char_image_path = os.path.join(save_path, f\"{char_count}.png\")\n",
    "        cv2.imwrite(char_image_path, char_image)\n",
    "\n",
    "        # Update count\n",
    "        char_counts[char] = char_count+1\n",
    "\n",
    "# Force character extraction even if results are already available\n",
    "FORCE_EXTRACT_CHAR = False\n",
    "\n",
    "char_counts = {}\n",
    "# Extract and save images for characters\n",
    "if FORCE_EXTRACT_CHAR or not os.path.exists(CHAR_IMAGE_FOLDER):\n",
    "    for captcha_image, captcha_text in zip(captcha_images_tv, captcha_texts_tv):\n",
    "        # Extract character images\n",
    "        char_images = extract_chars(captcha_image)\n",
    "        # Skip if extraction failed\n",
    "        if char_images is None:\n",
    "            continue\n",
    "        # Save character images\n",
    "        save_chars(char_images, captcha_text, CHAR_IMAGE_FOLDER, char_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95AUkry-pRFM"
   },
   "source": [
    "## Label Encoding\n",
    "During the training stage, we are going to load character images from previous stages as features and generate corresponding labels from their path. We will then rescale features, one-hot encode labels (occurred characters) and save labels to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3O_ZNwMpRFM"
   },
   "outputs": [],
   "source": [
    "# Path of occurred characters (labels)\n",
    "LABELS_PATH = \"./labels.pkl\"\n",
    "\n",
    "def make_feature(image):\n",
    "    \"\"\" Process character image and turn it into feature. \"\"\"\n",
    "    # Resize letter to 20*20\n",
    "    image_resized = resize_to_fit(image, 20, 20)\n",
    "    # Add extra dimension as the only channel\n",
    "    feature = image_resized[..., None]\n",
    "\n",
    "    return feature\n",
    "\n",
    "def make_feature_label(image_path):\n",
    "    \"\"\" Load character image and make feature-label pair from image path. \"\"\"\n",
    "    # Load image and make feature\n",
    "    feature = make_feature(cv2.imread(image_path, cv2.COLOR_BGR2GRAY))\n",
    "    # Extract label based on the directory the image is in\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "\n",
    "    return feature, label\n",
    "\n",
    "# Make features and labels from character image paths\n",
    "features_tv, labels_tv = unzip((\n",
    "    make_feature_label(image_path) for image_path in paths.list_images(CHAR_IMAGE_FOLDER)\n",
    "))\n",
    "\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_tv = np.array(features_tv, dtype=\"float\")/255\n",
    "# Convert labels into one-hot encodings\n",
    "lb = LabelBinarizer()\n",
    "labels_one_hot_tv = lb.fit_transform(labels_tv)\n",
    "# Number of classes\n",
    "n_classes = len(lb.classes_)\n",
    "\n",
    "# Further split the training data into training and validation set\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(\n",
    "    features_tv, labels_one_hot_tv, test_size=0.25, random_state=955996\n",
    ")\n",
    "# Save mapping from labels to one-hot encoding\n",
    "with open(LABELS_PATH, \"wb\") as f:\n",
    "    pickle.dump(lb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaeT46sppRFM"
   },
   "source": [
    "# Training\n",
    "Next, we build a Convolutional Neural Network (CNN) as our classification model with Tensorflow / Keras. The structure of the neural network is shown below:\n",
    "\n",
    "![conv-net-structure.png](attachment:conv-net-structure.png)\n",
    "\n",
    "After building the neural network, we train it and save weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfjwrBjApRFM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "# Number of epochs\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Path of model weights file\n",
    "MODEL_WEIGHTS_PATH = \"./captcha-model.weights.h5\"\n",
    "# Force training even if weights are already available\n",
    "FORCE_TRAINING = True\n",
    "\n",
    "# Build a feed-forward neural network\n",
    "model = Sequential()\n",
    "\n",
    "## [ TODO ]\n",
    "# Implement the neural network shown above\n",
    "\n",
    "# First convolution block: (*, 20, 20, 1) -Conv2D+ReLU-> (*, 20, 20, 20) -MaxPooling2D-> (*, 10, 10, 20)\n",
    "# (Remember to include input shape for the first layer!)\n",
    "# 1) Convolution layer: 20 channels, 5*5 kernel, ReLU activation, padded to maintain same shape\n",
    "# 2) Max pooling layer: 2*2 kernel\n",
    "model.add(NotImplemented)\n",
    "model.add(NotImplemented)\n",
    "\n",
    "# Second convolution block: (*, 10, 10, 20) -Conv2D+ReLU-> (*, 10, 10, 50) -MaxPooling2D-> (*, 5, 5, 50)\n",
    "# Convolution layer: same as above, but use 50 channels\n",
    "model.add(NotImplemented)\n",
    "model.add(NotImplemented)\n",
    "\n",
    "# Flatten layer reshape features to 1D: (*, 5, 5, 50) -Flatten-> (*, 1250)\n",
    "model.add(NotImplemented)\n",
    "\n",
    "# First fully-connected (linear) layer: (*, 1250) -FC+ReLU -> (*, 500)\n",
    "model.add(NotImplemented)\n",
    "# Last fully-connected (linear) layer: (*, 500) -FC+Softmax-> (*, n_classes)\n",
    "# Softmax outputs a categorical distribution representing probability for each character\n",
    "model.add(NotImplemented)\n",
    "\n",
    "# Build classification model with Adam optimizer and cross entropy loss\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "if FORCE_TRAINING or not os.path.exists(MODEL_WEIGHTS_PATH):\n",
    "    # Train the neural network model\n",
    "    model.fit(\n",
    "        X_train, y_train, validation_data=(X_vali, y_vali),\n",
    "        batch_size=BATCH_SIZE, epochs=N_EPOCHS, verbose=1\n",
    "    )\n",
    "    # Save model weights to disk\n",
    "    model.save_weights(MODEL_WEIGHTS_PATH)\n",
    "else:\n",
    "    model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "\n",
    "# Show model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkHL_24FpRFN"
   },
   "source": [
    "# Evaluation\n",
    "During the training part, we have validated the performance of our neural network model on images of single characters. Now it's time to test and evaluate CAPTCHAs from the beginning to the end. First, we will need to build the pipeline for CAPTCHA character prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDa-M0RHpRFN"
   },
   "outputs": [],
   "source": [
    "# Load labels from file (so we can translate model predictions to actual letters)\n",
    "with open(LABELS_PATH, \"rb\") as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "# Test our pipeline (and model) with the test set.\n",
    "# However, you'd want to replace this with some random CAPTCHAs in the real world.\n",
    "\n",
    "# Dummy character images\n",
    "DUMMY_CHAR_IMAGES = np.zeros((4, 20, 20, 1))\n",
    "\n",
    "# Indices of CAPTCHAs on which extractions failed\n",
    "extract_failed_indices = []\n",
    "# Extracted character images\n",
    "char_images_test = []\n",
    "\n",
    "# Extract character images and make features\n",
    "for i, captcha_image in enumerate(captcha_images_test):\n",
    "    # Extract character images\n",
    "    char_images = extract_chars(captcha_image)\n",
    "\n",
    "    if char_images:\n",
    "        char_images_test.extend(char_images)\n",
    "    # Use dummy character images as placeholder if extraction failed\n",
    "    else:\n",
    "        extract_failed_indices.append(i)\n",
    "        char_images_test.extend(DUMMY_CHAR_IMAGES)\n",
    "\n",
    "# Make features for character images\n",
    "features_test = [make_feature(char_image) for char_image in char_images_test]\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_test = np.array(features_test, dtype=\"float\")/255\n",
    "\n",
    "# Make features for character images\n",
    "features_test = [make_feature(char_image) for char_image in char_images_test]\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_test = np.array(features_test, dtype=\"float\")/255\n",
    "# Predict labels with neural network\n",
    "preds_test = model.predict(features_test)\n",
    "# Convert one-hot encoded characters back\n",
    "preds_test = lb.inverse_transform(preds_test)\n",
    "\n",
    "# Group all 4 characters for the same CAPTCHA\n",
    "preds_test = [\"\".join(chars) for chars in group_every(preds_test, 4)]\n",
    "# Update result for CAPTCHAs on which extractions failed\n",
    "for i in extract_failed_indices:\n",
    "    preds_test[i] = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bv0LrtapRFN"
   },
   "source": [
    "Now, we can compute the accuracy of our pipeline, as well as taking a look at correct and incorrect CAPTCHA text predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuIAAvCypRFN"
   },
   "outputs": [],
   "source": [
    "# Number of CAPTCHAs to display\n",
    "N_DISPLAY_SAMPLES = 10\n",
    "\n",
    "# Number of test CAPTCHAs\n",
    "n_test = len(captcha_texts_test)\n",
    "# Number of correct predictions\n",
    "n_correct = 0\n",
    "\n",
    "# Indices of correct predictions\n",
    "correct_indices = []\n",
    "# Indices of incorrect predictions\n",
    "incorrect_indices = []\n",
    "\n",
    "for i, (pred_text, actual_text) in enumerate(zip(preds_test, captcha_texts_test)):\n",
    "    if pred_text==actual_text:\n",
    "        ## [ TODO ]\n",
    "        # 1) Update number of correct predictions\n",
    "        raise NotImplementedError\n",
    "        # 2) Collect index of correct prediction\n",
    "        if len(correct_indices)<N_DISPLAY_SAMPLES:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        # 3) Collect index of incorrect prediction\n",
    "        if len(incorrect_indices)<N_DISPLAY_SAMPLES:\n",
    "            raise NotImplementedError\n",
    "\n",
    "# Show number of total / correct predictions and accuracy\n",
    "print(\"# of test CAPTCHAs:\", n_test)\n",
    "print(\"# correctly recognized:\", n_correct)\n",
    "print(\"Accuracy:\", n_correct/n_test, \"\\n\")\n",
    "\n",
    "# Show all correct predictions\n",
    "print_images(\n",
    "    [captcha_images_test[i] for i in correct_indices],\n",
    "    texts=[f\"Correct: {captcha_texts_test[i]}\" for i in correct_indices],\n",
    "    n_rows=2\n",
    ")\n",
    "\n",
    "# Show all incorrect predictions\n",
    "print_images(\n",
    "    [captcha_images_test[i] for i in incorrect_indices],\n",
    "    texts=[\n",
    "        f\"Prediction: {preds_test[i]}\\nActual: {captcha_texts_test[i]}\" \\\n",
    "        for i in incorrect_indices\n",
    "    ],\n",
    "    n_rows=2,\n",
    "    fig_size=(20, 6),\n",
    "    text_center=(0.5, -0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI_CEMmteItr"
   },
   "source": [
    "### Open-Ended Extension:\n",
    "\n",
    "TensorFlow is one popular framework for implementing Deep Neural Networks, and another widely used framework is **PyTorch**. After you successfully complete this lab:\n",
    "\n",
    "- Use your favorite language model and/or coding assistant (e.g., **GPT**, **Copilot**, **Gemini**, **Cursor**, etc.) to **convert your TensorFlow CNN implementation into PyTorch**.\n",
    "- Repeat the same training + evaluation experiments in PyTorch.\n",
    "- You should observe **similar results** (minor differences are expected due to randomness and implementation details).\n",
    "\n",
    "âœ… **Submission Requirement:**  \n",
    "Include this as a **separate notebook** in your lab-submission GitHub repository named:\n",
    "\n",
    "**`Breaking-CAPTCHAS-Pytorch.ipynb`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsW5uxopRFN"
   },
   "source": [
    "## References\n",
    "1. How to break a CAPTCHA system in 15 minutes with Machine Learning: https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\n",
    "2. CaptchaSolver Jupyter Notebook: https://github.com/BenjaminWegener/CaptchaSolver\n",
    "3. Keras Tutorial: The Ultimate Beginner's Guide to Deep Learning in Python: https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "4. Keras API reference: https://keras.io/\n",
    "5. Tensorflow API reference: https://www.tensorflow.org/api_docs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "anaconda-ml-ai",
   "language": "python",
   "name": "conda-env-anaconda-ml-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
